# ONNX-NLP-DNN-Perf

## ğŸš€ Project Overview
This project demonstrates the importance of integrating tokenization directly into ONNX-based Deep Neural Network (DNN) models for NLP tasks. By doing so, we improve inference speed, optimize memory usage, and eliminate the need for separate preprocessing steps.

## ğŸ“Œ Why This Project?
- **Traditional DNN models (like OpenCV's DNN module) do not handle textual data.**
- **ONNX bridges the gap, enabling NLP models to work efficiently within a DNN pipeline.**
- **By integrating tokenization inside the model, we achieve faster inference and better memory optimization.**

## ğŸ› ï¸ Required Libraries
Ensure you have the following dependencies installed before running the project:

```bash
pip install onnxruntime transformers datasets numpy opencv-python
```

## ğŸ“‚ How to Set Up & Run
1. **Clone the repository:**
   ```bash
   [git clone https://github.com/your-username/ONNX-NLP-DNN-Perf.git](https://github.com/PramodBasavarajMenasi/ONNX-NLP-DNN-Perf-.git)
   cd ONNX-NLP-DNN-Perf
   ```

## ğŸ“Š Results

| Scenario              | Execution Time  | Memory Usage |
|----------------------|----------------|--------------|
| **Before Tokenization Integration** | 2.5495 seconds | 53.31 KB |
| **After Tokenization Integration**  | 0.3809 seconds | 46.17 KB |

## ğŸ“¢ Key Takeaways
- **Without tokenization inside the model:** Higher execution time and memory overhead.
- **With tokenization inside the model:** Faster inference and reduced memory usage.


